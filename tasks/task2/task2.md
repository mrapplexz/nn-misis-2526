# ДЗ №2

Есть обученный Transformer Decoder. Нужно реализовать разные способы генерации текста для заранее обученного Transformer Decoder.

Пользоваться функцией `generate` из `transformers` нельзя, но можно подглядывать в неё, чтобы сделать свою имплементацию.

## Подготовка

Создать экземпляр модели и токенизатора можно при помощи библиотеки `transformers`: 

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-0.5B-Instruct').eval()
tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-0.5B-Instruct')
```

В качестве текста для токенизации мы будем использовать специальный шаблон - промпт. Модель была обучена на множестве текстов, шаблонизированных таким образом. Для выполнения заданий будем использовать два промпта. 

Первый - попросим модель сгенерировать короткую сказку про ёжика по имени Соник.

Второй - попросим модель генерировать структурированный JSON `{"contractor": string, "sum": decimal, "currency": string}` по пользовательскому запросу `Transfer 100 rubles and 50 kopeck to Mike`.

```python
input_text_hedgehog = '<|im_start|>system\nYou are a storyteller. Generate a story based on user message.<|im_end|>\n<|im_start|>user\nGenerate me a short story about a tiny hedgehog named Sonic.<|im_end|>\n<|im_start|>assistant\n'
input_text_json = '<|im_start|>system\nYou are a JSON machine. Generate a JSON with format {"contractor": string with normalized contractor name, "sum": decimal, "currency": string with uppercased 3-letter currency code} based on user message.<|im_end|>\n<|im_start|>user\nTransfer 100 rubles and 50 kopeck to Mike<|im_end|>\n<|im_start|>assistant\n'
```

Получить объект `Encoding` (в котором содержатся `ids` и `attention_mask`, используя `AutoTokenizer`, можно следующим образом:

```python
encoding = tokenizer(input_text).encodings[0]
```

Модель в качестве inputs может принимать `input_ids` и `attention_mask`, которые, в свою очередь, можно получить путём токенизации. Возвращает модель логиты (числа ДО применения softmax):

```python
logits = model(input_ids=..., attention_mask=...).logits
logits_for_first_batch_and_last_token = logits[0, -1]
```

Наконец, когда генерация завершена, декодировать токены в текст можно при помощи токенизатора:

```python
output_text = tokenizer.decode([4913, 20257, 269, 788, 330, 34441, 497, 330, 1242, 788, 220, 16, 15, 20, 11, 330, 15973, 788, 330, 59430, 9207, 151645], skip_special_tokens=False)
```

### P.S.

Модель `Qwen2.5-0.5B-Instruct` является немного модифицированной реализацией Transformer Decoder, который мы уже имплементировали на занятии. Подробное объяснение всех доработок в Transformer будет на 4 курсе в рамках углублённого NLP-курса, но кому интересно, уже сейчас можно почитать код: https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/modeling_qwen2.py . Для решения задач необязательно разбирать исходный код модели, достаточно работать с её входами и выходами.

## Задача 1. Greedy Decoding

На каждом шаге генерации нужно выбирать самый вероятный токен.

Заканчивать генерацию, если выполнилось одно из двух условий:
1. сгенерировался EOS-токен с ID = `151645`
2. длина генерации превысила 1000 токенов.

В результатах:
1. Ответить на вопрос: если запустить алгоритм несколько раз, то будут ли различаться генерации?
2. Ответить на вопрос: какие есть проблемы с таким подходом к генерации в случае с генерацией сказки и в случае с генерацией JSON?
3. Приложить сгенерированный текст про ёжика и сгенерированный JSON.


## Задача 2. Sampling

На каждом шаге генерации можно получить из модели распределение вероятностей для следующего токена. Нужно выбирать не самый вероятный токен, а выполнять сэмплирование среди всех токенов из этого распределения. Функция `torch.multinomial` в помощь.

Заканчивать генерацию, если выполнилось одно из двух условий:
1. сгенерировался EOS-токен с ID = `151645`
2. длина генерации превысила 1000 токенов.

В результатах:
1. Ответить на вопрос: если запустить алгоритм несколько раз, то будут ли различаться генерации?
2. Ответить на вопрос: какие есть проблемы с таким подходом к генерации в случае с генерацией сказки и в случае с генерацией JSON?
3. Приложить 3 сгенерированных текста про ёжика и 3 сгенерированных JSON.


## Задача 3. Sampling meets Temperature

Как прошлая задача, но нужно применить температуру (до взятия softmax).

В результатах:
1. Ответить на вопросы: как отличаются генерации с температурами: `0.001`, `0.1`, `0.5`, `1.0`, `10.0`? Есть ли какая-то закономерность при уменьшении/увеличении температуры? Для каких задач какая температура лучше?
2. Приложить сгенерированные текста про ёжика и сгенерированные JSON, по одному тексту про ёжика и одному JSON для каждой температуры из `0.001`, `0.1`, `0.5`, `1.0`, `10.0`.


## Задача 4. Nucleus Sampling

https://arxiv.org/pdf/1904.09751 - смотрите статью с занимательным названием "THE CURIOUS CASE OF NEURAL TEXT DeGENERATION"

Как прошлая задача, но нужно сэмплировать не из всего распределения, а только среди самых вероятных токенов.

Как это сделать:
1. Имеем распределение вероятностей по токенам, полученное от модели с применённой температурой.
2. Оставляем только самые вероятные токены, кумулятивная вероятность которых не превышает `top_p`. Остальные выбрасываем. Если так получилось, что самый вероятный токен уже имеет вероятность больше, чем `top_p`, - оставляем только его.
3. Так как вектор из вероятностей теперь не совсем распределение (значения не суммируются в `1`), то отмасштабируем каждую вероятность, разделив её на сумму всех вероятностей. Теперь значения суммируется в `1`. Смотрите формулу #3 в статье.
4. Выполняем сэмплирование.

В результатах:
1. Ответить на вопросы: как отличаются генерации с 1) `temperature=1`, `top_p=0.9`; 2) `temperature=1`, `top_p=0.15`; 3) `temperature=0.5`, `top_p=0.9`, 4) `temperature=0.5`, `top_p=0.15`?
2. Помог ли nucleus sampling исправить какие-то проблемы, которые были при простом сэмплировании с температурой? 
3. Приложить сгенерированные текста про ёжика и сгенерированные JSON, по одному тексту про ёжика и одному JSON для каждого набора параметров: 1) `temperature=1`, `top_p=0.9`; 2) `temperature=1`, `top_p=0.15`; 3) `temperature=0.5`, `top_p=0.9`, 4) `temperature=0.5`, `top_p=0.15`.


## Задача 5. Early-Stopped Beam Search

Эта задача довольно похожа на первую. 

Но выгодно ли выбирать самый вероятный токен? Разве не может получиться так, что на шаге `i` мы выберем самый вероятный токен с `p=0.6`, который породит следующий самый вероятный токен с `p=0.1`? Тогда совместная вероятность такой последовательности `0.06`. А если бы мы выбрали второй по вероятности токен на шаге `i` с `p=0.3`, который породил бы следующий самый вероятный токен с `p=0.9`? Тогда получили бы совместную вероятность `0.27`.

В общем, **выбор самого вероятного токена не всегда приведёт нас к самой вероятной последовательности**. Давайте реализуем алгоритм beam search, чтобы уметь "подглядывать вперёд".

Как это можно реализовать:

1. Зададим параметры `num_beams: int` и `length_penalty: float`
2. В памяти будем держать список **незаконченных кандидатов**. Кандидат - это объект, состоящий из **последовательности токенов** и из **численного скора**. Изначально этот список будет пустым.
3. В памяти будем держать список **законченных кандидатов**. Изначально этот список будет пустым.
4. Прогоним через модель исходную последовательность, возьмём `num_beams` самых вероятных продолжений. Если среди продолжений есть EOS-токен - добавим продолжение в список **законченных кандидатов**. Все остальные продолжения добавим в **список незаконченных кандидатов**. Для каждого кандидата скор будет равен значению соответствующего `logprobe` (результат применения функции `log_softmax`  на `logit`).
5. Каждый элемент в **списке незаконченных кандидатов** прогоним через модель, возьмём `num_beams` самых вероятных продолжений для каждого кандидата. Получим `len(candidates) * num_beams` последовательностей. Скор для каждого кандидата будет равен сумме скора кандидата-родителя и значению `logprobe` у добавленного токена-продолжения.
6. Отранжируем полученный список по `candidate_score` от большего к меньшему.
7. Если среди первых `num_beams` элементов полученного списка есть законченные кандидаты - добавим их к **списку законченных кандидатов** и удалим из текущего списка. Оставшиеся первые `num_beams` элементов запишем в список **незаконченных кандидатов** (полностью перезапишем этот список).
8. Повторим шаг 5.

В указанном алгоритме нужно заканчивать генерацию, если набралось `num_beams` **законченных кандидатов**. Из **законченных кандидатов** нужно выбрать одну последовательность, имеющую самый высокий скор по формуле ранжирования: `candidate_score / (candidate_length ** length_penalty)`.

Визуализация алгоритма: https://huggingface.co/spaces/m-ric/beam_search_visualizer

В результатах:
1. Ответить на вопросы: как отличаются результаты с 1) `num_beams=1`, `length_penalty=1.0`; 2) `num_beams=4`, `length_penalty=1.0`; 3) `num_beams=4`, `length_penalty=0.5`; 4) `num_beams=4`, `length_penalty=2.0`; 5) `num_beams=8`, `length_penalty=1.0`? Есть ли какая-то закономерность при увеличении/уменьшении `num_beams` и `length_penalty`?
2. Ответить на вопросы: Помог ли текущий способ исправить проблемы, которые возникли с Greedy Decoding? Для какого рода задач beam search подходит больше, чем nucleus sampling?
3. Приложить сгенерированные текста про ёжика и сгенерированные JSON, по одному тексту про ёжика и одному JSON для каждого набора параметров: 1) `num_beams=1`, `length_penalty=1.0`; 2) `num_beams=4`, `length_penalty=1.0`; 3) `num_beams=4`, `length_penalty=0.5`; 4) `num_beams=4`, `length_penalty=2.0`; 5) `num_beams=8`, `length_penalty=1.0`.


## Сдача

Код для воспроизведения результатов (чтобы можно было запустить разные .py-файлы и воспроизвести результаты) в виде ссылки на GitHub репозиторий нужно отправить мне в личку в телеграм.
В readme.md в репозитории укажите примеры генерации и ответы на вопросы с небольшим пояснением для каждой задачи.

### Баллы
Если сделаете только 1 задачу - можно получить 3 балла. Если 1-3 - 4 балла. Если 1-5 - 5 баллов.

### Дедлайн

Сдавать и корректировать решение можно до 30 апреля.
